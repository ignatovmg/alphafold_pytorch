{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.functional as F\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4],\n",
       "         [0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4]]])"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.arange(5)\n",
    "a.tile((1, 2, 4)).split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 0., 0.],\n",
       "         [0., 1., 0.],\n",
       "         [0., 0., 1.],\n",
       "         [0., 0., 0.]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.]]))"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.eye(4).split([3, 1], dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import torch\n",
    "\n",
    "\n",
    "# Array of 3-component vectors, stored as individual array for\n",
    "# each component.\n",
    "Vecs = collections.namedtuple('Vecs', ['x', 'y', 'z'])\n",
    "\n",
    "# Array of 3x3 rotation matrices, stored as individual array for\n",
    "# each component.\n",
    "Rots = collections.namedtuple('Rots', ['xx', 'xy', 'xz',\n",
    "                                       'yx', 'yy', 'yz',\n",
    "                                       'zx', 'zy', 'zz'])\n",
    "# Array of rigid 3D transformations, stored as array of rotations and\n",
    "# array of translations.\n",
    "Rigids = collections.namedtuple('Rigids', ['rot', 'trans'])\n",
    "\n",
    "\n",
    "def squared_difference(x, y):\n",
    "    return torch.square(x - y)\n",
    "\n",
    "\n",
    "def apply_tree(fun, *r: Rigids) -> Rigids:\n",
    "    return Rigids(\n",
    "        rot=Rots(*[fun(*[x.rot[i] for x in r]) for i in range(9)]),\n",
    "        trans=Vecs(*[fun(*[x.trans[i] for x in r]) for i in range(3)])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "rig = Rigids(rot=Rots(*([np.ones((20, 5))] * 9)), trans=Vecs(*([np.ones((20, 5))] * 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Rigids(rot=Rots(xx=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), xy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), xz=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), yx=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), yy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), yz=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), zx=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), zy=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), zz=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])), trans=Vecs(x=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), y=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]), z=array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1.])))"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apply_tree(lambda x, y: np.concatenate([x[0], y[1]]), rig, rig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.tensor([[10, 30, 20], [60, 40, 50]])\n",
    "t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([36.0958, 43.5799, -1.4710])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor([0.8850, 0, 0, 0.4445, 0, 0, -0.1384, 0, 0]).reshape(3, 3)\n",
    "torch.matmul(mat, torch.tensor((-0.520, 1.363, 0.000))) + torch.tensor([36.5560, 43.8110, -1.5430])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.],\n",
       "        [0., 0., 0.],\n",
       "        [0., 0., 0.]])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(mat < mat).type(mat.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[9.9997e-01, 5.0455e-05, 3.7394e-05],\n",
       "        [5.0455e-05, 1.0000e+00, 4.1932e-05],\n",
       "        [3.7394e-05, 4.1932e-05, 1.0001e+00]])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = torch.tensor(\n",
    "   [[-0.8796,  0.3340, -0.3387],\n",
    "          [-0.4209, -0.2145,  0.8814],\n",
    "          [ 0.2217,  0.9179,  0.3293]]\n",
    ")\n",
    "torch.matmul(mat, mat.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5],\n",
       "         [0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5]]])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.arange(6)\n",
    "tmp.tile(1, 2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 1., 0., 3.])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.arange(4)[:, None] * mat).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 3)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat = np.zeros((3, 4))\n",
    "mat.swapaxes(0, 1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[132, 133, 134],\n",
       "         [132, 133, 134],\n",
       "         [138, 139, 140]],\n",
       "\n",
       "        [[147, 148, 149],\n",
       "         [144, 145, 146],\n",
       "         [147, 148, 149]]])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_atom_positions = torch.arange(2 * 37 * 3).reshape(2, 37, 3)\n",
    "residx_rigidgroup_base_atom37_idx = torch.tensor([\n",
    "    [[1,2,3], [6,6,6]],\n",
    "    [[7,7,9], [12,11,12]]\n",
    "])\n",
    "torch.gather(\n",
    "    all_atom_positions.unsqueeze(-2).tile([1, 1, 3, 1]), 1, \n",
    "    residx_rigidgroup_base_atom37_idx.unsqueeze(-1).tile([1, 1, 1, 3])\n",
    ")[1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 37, 2, 3])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_atom_positions.unsqueeze(-2).tile([1, 1, 2, 1]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3, 1])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residx_rigidgroup_base_atom37_idx.unsqueeze(-1).shape #.tile([1, 1, 1, 2]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "residx_rigidgroup_base_atom37_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728]], grad_fn=<AddmmBackward>),\n",
       " 'b': tensor([[-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728],\n",
       "         [-0.0916,  0.0089,  0.1360,  0.3220,  0.1380,  0.3717,  0.1188, -0.3970,\n",
       "          -0.2157, -0.2728]], grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Test(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.l = nn.Linear(10, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return {'a': self.l(x['a']), 'b': self.l(x['a'])}\n",
    "    \n",
    "    \n",
    "model = nn.Sequential(Test(), Test(), Test())\n",
    "model({'a': torch.zeros(5, 10), 'b': torch.zeros(5, 6, 10)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_1 = {'a': torch.zeros(5, 10), 'b': torch.zeros(5, 6, 10)}\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_1 = model(inputs_1)\n",
    "    \n",
    "output_2 = model(output_1)\n",
    "#inputs_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048]]),\n",
       " 'b': tensor([[ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048],\n",
       "         [ 0.1672, -0.0184, -0.1907,  0.1110,  0.3153,  0.2358,  0.1021,  0.0036,\n",
       "          -0.2361, -0.1048]])}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': tensor([[ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460]], grad_fn=<AddmmBackward>),\n",
       " 'b': tensor([[ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460],\n",
       "         [ 0.1213, -0.1375, -0.3580,  0.2230,  0.2943,  0.0403,  0.1334,  0.0267,\n",
       "          -0.1734, -0.2460]], grad_fn=<AddmmBackward>)}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_2['a'].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'generator' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-2c017e5737f0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'generator' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "model.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((6,9))\n",
    "a[cprod[:, 0], cprod[:, 1]] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 10, 10])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.ones((2, 10, 5))\n",
    "b = torch.ones((2, 4, 10, 5))\n",
    "torch.einsum('bic,bmjc->bmij', a, b).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 10, 15, 5, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = torch.einsum('bmix,bmjy->bmijxy', torch.ones((2, 1, 10, 5)), torch.ones((2, 3, 15, 5)))\n",
    "tmp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10, 10, 25])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp.view(*tmp.shape[:-2], -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'rec_profile': {\n",
    "        'num_c': 64\n",
    "    },\n",
    "    'lig_profile': {\n",
    "        'num_c': 64\n",
    "    },\n",
    "    'pair_rep': {\n",
    "        'num_c': 64\n",
    "    },\n",
    "    'SeqRowAttentionWithPairBias': {\n",
    "        'attention_num_c': 32,\n",
    "        'num_heads': 8\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "class SeqRowAttentionWithPairBias(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        attn_num_c = config['SeqRowAttentionWithPairBias']['attention_num_c']\n",
    "        num_heads = config['SeqRowAttentionWithPairBias']['num_heads']\n",
    "        rec_num_c = config['rec_profile']['num_c']\n",
    "        lig_num_c = config['lig_profile']['num_c']\n",
    "        pair_rep_num_c = config['pair_rep']['num_c']\n",
    "        self.rec_q = nn.Linear(rec_num_c, attn_num_c * num_heads, bias=False)\n",
    "        self.rec_k = nn.Linear(rec_num_c, attn_num_c * num_heads, bias=False)\n",
    "        self.rec_v = nn.Linear(rec_num_c, attn_num_c * num_heads, bias=False)\n",
    "        self.lig_q = nn.Linear(lig_num_c, attn_num_c * num_heads, bias=False)\n",
    "        self.lig_k = nn.Linear(lig_num_c, attn_num_c * num_heads, bias=False)\n",
    "        self.lig_v = nn.Linear(lig_num_c, attn_num_c * num_heads, bias=False)\n",
    "\n",
    "        self.rec_rec_project = nn.Linear(pair_rep_num_c, num_heads)\n",
    "        self.lig_lig_project = nn.Linear(pair_rep_num_c, num_heads)\n",
    "        self.rec_lig_project = nn.Linear(pair_rep_num_c, num_heads)\n",
    "        \n",
    "        self.rec_final = nn.Linear(attn_num_c * num_heads, rec_num_c)\n",
    "        self.lig_final = nn.Linear(attn_num_c * num_heads, lig_num_c)\n",
    "        \n",
    "        self.attn_num_c = attn_num_c\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, rec_profile, lig_profile, rec_rec, lig_lig, rec_lig, lig_rec):\n",
    "        batch_size = rec_profile.shape[0]\n",
    "        num_res = rec_profile.shape[1]\n",
    "        num_atoms = lig_profile.shape[2]\n",
    "        num_cep = lig_profile.shape[1]\n",
    "        \n",
    "        rec_q = self.rec_q(rec_profile).view(*rec_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        rec_k = self.rec_k(rec_profile).view(*rec_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        rec_v = self.rec_v(rec_profile).view(*rec_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        lig_q = self.lig_q(lig_profile).view(*lig_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        lig_k = self.lig_k(lig_profile).view(*lig_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        lig_v = self.lig_v(lig_profile).view(*lig_profile.shape[:-1], self.attn_num_c, self.num_heads)\n",
    "        \n",
    "        weights = torch.zeros((batch_size, num_cep, num_res + num_atoms, num_res + num_atoms, self.num_heads), device=rec_profile.device, dtype=rec_profile.dtype)\n",
    "\n",
    "        #print(rec_q.shape)\n",
    "        #print(lig_k.shape)\n",
    "        rec_rec_aff = torch.einsum('bich,bjch->bijh', rec_q, rec_k)\n",
    "        rec_lig_aff = torch.einsum('bich,bmjch->bmijh', rec_q, lig_k)\n",
    "        lig_rec_aff = torch.einsum('bich,bmjch->bmjih', rec_k, lig_q)\n",
    "        lig_lig_aff = torch.einsum('bmich,bmjch->bmijh', lig_q, lig_k)\n",
    "\n",
    "        rec_rec_bias = self.rec_rec_project(rec_rec).view(*rec_rec.shape[:-1], self.num_heads)\n",
    "        lig_lig_bias = self.lig_lig_project(lig_lig).view(*lig_lig.shape[:-1], self.num_heads)\n",
    "        rec_lig_bias = self.rec_lig_project(rec_lig).view(*rec_lig.shape[:-1], self.num_heads)\n",
    "        lig_rec_bias = self.rec_lig_project(lig_rec).view(*lig_rec.shape[:-1], self.num_heads)\n",
    "        \n",
    "        #print(rec_rec_aff.shape)\n",
    "        #print(rec_rec_bias.shape)\n",
    "        print(weights.shape)\n",
    "        weights[:, :, :num_res, :num_res] = rec_rec_aff + rec_rec_bias\n",
    "        weights[:, :, :num_res, num_res:] = rec_lig_aff + rec_lig_bias\n",
    "        weights[:, :, num_res:, :num_res] = lig_rec_aff + lig_rec_bias\n",
    "        weights[:, :, num_res:, num_res:] = lig_lig_aff + lig_lig_bias\n",
    "        weights = torch.softmax(weights, dim=-2)\n",
    "        #print(weights)\n",
    "\n",
    "        rec_profile = torch.einsum('brch,birh->bich', rec_v, weights[:, 0, :num_res, :num_res]) + torch.einsum('bmrch,bmirh->bmich', lig_v, weights[:, :, :num_res, num_res:]).mean(1)\n",
    "        lig_profile = torch.einsum('bmrch,bmirh->bmich', lig_v, weights[:, :, num_res:, num_res:]) + torch.einsum('brch,bmirh->bmich', rec_v, weights[:, :, num_res:, :num_res])\n",
    "        \n",
    "        rec_profile = self.rec_final(rec_profile.reshape(*rec_profile.shape[:-2], -1))\n",
    "        lig_profile = self.lig_final(lig_profile.reshape(*lig_profile.shape[:-2], -1))\n",
    "        return rec_profile, lig_profile\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = SeqRowAttentionWithPairBias(config)\n",
    "#layer.to('cuda:0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(132760)"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([torch.prod(torch.tensor(x.shape)) for x in layer.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_res = 100\n",
    "num_atoms = 12\n",
    "num_msa = 128\n",
    "a = torch.ones(1, num_res, config['rec_profile']['num_c'])\n",
    "b = torch.ones(1, num_msa, num_atoms, config['lig_profile']['num_c'])\n",
    "rec_rec = torch.ones(1, num_res, num_res, config['pair_rep']['num_c'])\n",
    "lig_lig = torch.ones(1, num_atoms, num_atoms, config['pair_rep']['num_c'])\n",
    "rec_lig = torch.ones(1, num_res, num_atoms, config['pair_rep']['num_c'])\n",
    "lig_rec = torch.ones(1, num_atoms, num_res, config['pair_rep']['num_c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 128, 112, 112, 8])\n",
      "tensor([[[[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]],\n",
      "\n",
      "\n",
      "         [[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]],\n",
      "\n",
      "\n",
      "         [[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]],\n",
      "\n",
      "\n",
      "         ...,\n",
      "\n",
      "\n",
      "         [[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]],\n",
      "\n",
      "\n",
      "         [[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]],\n",
      "\n",
      "\n",
      "         [[[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          [[9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           [9.9821e-03, 9.6372e-03, 5.8093e-04,  ..., 9.9995e-03,\n",
      "            9.0744e-03, 6.6431e-03],\n",
      "           ...,\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02],\n",
      "           [1.4957e-04, 3.0234e-03, 7.8492e-02,  ..., 4.1494e-06,\n",
      "            7.7135e-03, 2.7974e-02]],\n",
      "\n",
      "          ...,\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]],\n",
      "\n",
      "          [[9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           [9.8265e-03, 3.7070e-03, 6.3752e-03,  ..., 9.6276e-03,\n",
      "            3.1480e-03, 9.8109e-03],\n",
      "           ...,\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03],\n",
      "           [1.4462e-03, 5.2441e-02, 3.0207e-02,  ..., 3.1037e-03,\n",
      "            5.7100e-02, 1.5760e-03]]]]], grad_fn=<SoftmaxBackward>)\n"
     ]
    }
   ],
   "source": [
    "a, b = layer(a, b, rec_rec, lig_lig, rec_lig, lig_rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 12, 64])"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-211-346be9597376>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/projects/ml_env/venv/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/projects/ml_env/venv/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    145\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "b.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f5874995cd0>"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer.rec_k."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = MultiheadLinear(10, 20, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultiheadLinear(in_features=10, out_features=20, num_heads=4, bias=True)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 1, 2, 3])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l(torch.ones(1,2,3,10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_env",
   "language": "python",
   "name": "ml_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
