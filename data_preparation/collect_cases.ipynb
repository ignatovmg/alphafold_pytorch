{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [14:54:46] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from path import Path\n",
    "import json\n",
    "import seaborn as sns\n",
    "import prody\n",
    "from multiprocessing import Pool\n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import json\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "from io import StringIO\n",
    "from collections import OrderedDict, Counter\n",
    "import traceback\n",
    "import urllib\n",
    "import pybel\n",
    "from copy import deepcopy\n",
    "from multiprocessing import Pool\n",
    "from pymol import cmd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "from pocketdock.ligand.ligand_expo import LigandExpo\n",
    "from pocketdock import utils\n",
    "from pocketdock import pdb_tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import logging.config\n",
    "\n",
    "LOGGING = {\n",
    "    'version': 1,\n",
    "    'disable_existing_loggers': True,\n",
    "    'formatters': {\n",
    "        'default': {\n",
    "            'format': '[%(levelname)s] %(asctime)s %(funcName)s [pid %(process)d] - %(message)s'\n",
    "        }\n",
    "    },\n",
    "    'handlers': {\n",
    "        'console': {\n",
    "            'class': 'logging.StreamHandler',\n",
    "            'formatter': 'default',\n",
    "        }\n",
    "    },\n",
    "    'loggers': {\n",
    "        'console': {\n",
    "            'handlers': ['console'],\n",
    "            'level': 'DEBUG',\n",
    "            'propagate': False,\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "logging.config.dictConfig(LOGGING)\n",
    "logger = logging.getLogger('console')\n",
    "\n",
    "\n",
    "def get_file_handler(filename, mode='w', level='DEBUG'):\n",
    "    h = logging.FileHandler(filename, mode=mode)\n",
    "    h.setFormatter(logging.Formatter('[%(levelname)s] %(asctime)s %(funcName)s [pid %(process)d] - %(message)s'))\n",
    "    h.setLevel(level)\n",
    "    return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAIN_INTERACTION_DIST = 6\n",
    "MIN_POCKET_SIZE = 10\n",
    "MAX_RESOLUTION = 4.0\n",
    "LIGAND_GROUP_DIST = 5\n",
    "MIN_LIGAND_TO_SYMMATES = 6\n",
    "BSITE_SIM_RADIUS = 6\n",
    "LIGAND_DOMAIN_CONTACT_RADIUS = 6\n",
    "MAX_NUM_CHAINS = 20\n",
    "COVALENT_LIGAND_DISTANCE = 1.8\n",
    "FRAC_HEAVY_ATOMS_RESOLVED = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _build_d2mat(crd1, crd2):\n",
    "    return np.sum((crd1[:, None, :] - crd2[None, :, :])**2, axis=2)\n",
    "            \n",
    "def _calc_overlap(lig_ag, ref_ag, dist):\n",
    "    dmat = prody.buildDistMatrix(lig_ag.heavy, ref_ag.heavy)\n",
    "    return np.any(dmat <= dist, axis=1).sum().item() / lig_ag.heavy.numAtoms()\n",
    "\n",
    "def _aln_to_mapping(aln1, aln2):\n",
    "    assert len(aln1) == len(aln2)\n",
    "    i = 0\n",
    "    j = 0\n",
    "    mapping = []\n",
    "    for a, b in zip(aln1, aln2):\n",
    "        if a != '-' and b != '-':\n",
    "            mapping.append((i, j))\n",
    "        if a != '-':\n",
    "            i += 1\n",
    "        if b != '-':\n",
    "            j += 1\n",
    "    return mapping\n",
    "\n",
    "def _get_affinity(pdb_id, chemid):\n",
    "    try:\n",
    "        url = 'https://data.rcsb.org/rest/v1/core/entry/' + pdb_id.lower()\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            affs = json.loads(f.read().decode())\n",
    "    except urllib.error.HTTPError as e:\n",
    "        print('HTTPError for', url)\n",
    "        affs = {}\n",
    "        \n",
    "    for x in affs.get('rcsb_binding_affinity', []):\n",
    "        if x['comp_id'] == chemid:\n",
    "            return x\n",
    "    return None\n",
    "\n",
    "def calc_identity(aln1, aln2):\n",
    "    return sum([x == y for x, y in zip(aln1, aln2)]) / len(aln1.replace('-', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cases_for_pdb('1QUR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "three2one = {'SER': 'S', 'LYS': 'K', 'PRO': 'P', 'ALA': 'A', 'ASP': 'D', 'ARG': 'R', 'VAL': 'V', 'CYS': 'C', 'HIS': 'H',\n",
    "         'ASX': 'B', 'PHE': 'F', 'MET': 'M', 'LEU': 'L', 'ASN': 'N', 'TYR': 'Y', 'ILE': 'I', 'GLN': 'Q', 'THR': 'T',\n",
    "         'GLX': 'Z', 'GLY': 'G', 'TRP': 'W', 'GLU': 'E'}\n",
    "\n",
    "residue_bonds_noh = {\n",
    "    'GLY': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C')},\n",
    "    'ALA': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'), 'CB': ('CA',)},\n",
    "    'CYS': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'SG'), 'SG': ('CB',)},\n",
    "    'SER': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'OG'), 'OG': ('CB',)},\n",
    "    'MET': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'SD'), 'SD': ('CG', 'CE'), 'CE': ('SD',)},\n",
    "    'LYS': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD'), 'CD': ('CG', 'CE'), 'CE': ('CD', 'NZ'), 'NZ': ('CE',)},\n",
    "    'ARG': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD'), 'CD': ('CG', 'NE'), 'NE': ('CD', 'CZ'), 'CZ': ('NE', 'NH1', 'NH2'),\n",
    "            'NH1': ('CZ',), 'NH2': ('CZ',)},\n",
    "    'GLU': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD'), 'CD': ('CG', 'OE1', 'OE2'), 'OE1': ('CD',), 'OE2': ('CD',)},\n",
    "    'GLN': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD'), 'CD': ('CG', 'OE1', 'NE2'), 'OE1': ('CD',), 'NE2': ('CD',)},\n",
    "    'ASP': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'OD1', 'OD2'), 'OD1': ('CG',), 'OD2': ('CG',)},\n",
    "    'ASN': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'OD1', 'ND2'), 'OD1': ('CG',), 'ND2': ('CG',)},\n",
    "    'LEU': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD1', 'CD2'), 'CD1': ('CG',), 'CD2': ('CG',)},\n",
    "    'HIS': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'ND1', 'CD2'), 'ND1': ('CG', 'CE1'), 'CD2': ('CG', 'NE2'),\n",
    "            'CE1': ('ND1', 'NE2'), 'NE2': ('CD2', 'CE1')},\n",
    "    'PHE': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD1', 'CD2'), 'CD1': ('CG', 'CE1'), 'CD2': ('CG', 'CE2'),\n",
    "            'CE1': ('CD1', 'CZ'), 'CE2': ('CD2', 'CZ'), 'CZ': ('CE1', 'CE2')},\n",
    "    'TYR': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD1', 'CD2'), 'CD1': ('CG', 'CE1'), 'CD2': ('CG', 'CE2'),\n",
    "            'CE1': ('CD1', 'CZ'), 'CE2': ('CD2', 'CZ'), 'CZ': ('CE1', 'CE2', 'OH'), 'OH': ('CZ',)},\n",
    "    'TRP': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD1', 'CD2'), 'CD1': ('CG', 'NE1'), 'CD2': ('CG', 'CE2', 'CE3'),\n",
    "            'NE1': ('CD1', 'CE2'), 'CE2': ('CD2', 'NE1', 'CZ2'), 'CE3': ('CD2', 'CZ3'), 'CZ3': ('CE3', 'CH2'),\n",
    "            'CZ2': ('CE2', 'CH2'), 'CH2': ('CZ2', 'CZ3')},\n",
    "    'VAL': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG1', 'CG2'), 'CG1': ('CB',), 'CG2': ('CB',)},\n",
    "    'THR': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'OG1', 'CG2'), 'OG1': ('CB',), 'CG2': ('CB',)},\n",
    "    'ILE': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA',), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG1', 'CG2'), 'CG1': ('CB', 'CD1'), 'CD1': ('CG1',), 'CG2': ('CB',)},\n",
    "    'PRO': {'OXT': ('C',), 'C': ('CA', 'O', 'OXT'), 'O': ('C',), 'N': ('CA', 'CD'), 'CA': ('N', 'C', 'CB'),\n",
    "            'CB': ('CA', 'CG'), 'CG': ('CB', 'CD'), 'CD': ('N', 'CG')},\n",
    "}\n",
    "\n",
    "\n",
    "def fetch_AF_model(uniprot_id):\n",
    "    url = f'https://alphafold.ebi.ac.uk/files/AF-{uniprot_id}-F1-model_v1.pdb'\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        data = f.read().decode()\n",
    "    return prody.parsePDBStream(StringIO(data))\n",
    "\n",
    "\n",
    "def _find_closest_residue(resid_list, target, upstream=True):\n",
    "    if not upstream:\n",
    "        resid_list = reversed(resid_list)\n",
    "    for resid in resid_list:\n",
    "        if upstream and target <= resid:\n",
    "            return resid\n",
    "        if not upstream and target >= resid:\n",
    "            return resid\n",
    "    return None\n",
    "\n",
    "\n",
    "def fetch_domains(instance_data, entity_to_pdb, pdb_residues, entity_len):\n",
    "    data = instance_data['rcsb_polymer_instance_feature']\n",
    "    cath_data = [x for x in data if x['type'] == 'CATH']\n",
    "    entity_nums = sorted(entity_to_pdb.keys())\n",
    "    \n",
    "    domains = []\n",
    "    for dom_id, dom_data in enumerate(cath_data):\n",
    "        domain = OrderedDict(\n",
    "            domain_id=dom_id,\n",
    "            manually_filled=False,\n",
    "            domain_chunks=[]\n",
    "        )\n",
    "        for chunk_data in dom_data['feature_positions']:\n",
    "            # zero based position in the entity sequence\n",
    "            chunk_ent_orig = [int(chunk_data['beg_seq_id'])-1, int(chunk_data['end_seq_id'])-1]\n",
    "            # zero based position mapped to the closest residue with coordinates\n",
    "            chunk_ent_map = [\n",
    "                _find_closest_residue(entity_nums, chunk_ent_orig[0], True), \n",
    "                _find_closest_residue(entity_nums, chunk_ent_orig[1], False)\n",
    "            ]\n",
    "            # discard region if None\n",
    "            if chunk_ent_map[0] is None or chunk_ent_map[1] is None:\n",
    "                continue\n",
    "            # pdb residue ids\n",
    "            chunk_pdb = [\n",
    "                pdb_residues[entity_to_pdb[chunk_ent_map[0]]].getResnum().item() if chunk_ent_map[0] is not None else None,\n",
    "                pdb_residues[entity_to_pdb[chunk_ent_map[1]]].getResnum().item() if chunk_ent_map[1] is not None else None\n",
    "            ]\n",
    "            domain['domain_chunks'].append(\n",
    "                OrderedDict(\n",
    "                    chunk_ent_orig=chunk_ent_orig,\n",
    "                    chunk_ent_map=chunk_ent_map,\n",
    "                    chunk_pdb_resid=chunk_pdb\n",
    "                )\n",
    "            )\n",
    "        if len(domain['domain_chunks']) == 0:\n",
    "            continue\n",
    "        domains.append(domain)\n",
    "        \n",
    "    #assert len(domains) > 0, domains\n",
    "    \n",
    "    # if there is no domains identified, fill them manually - whole structure\n",
    "    if len(domains) == 0:\n",
    "        chunk_ent_orig = [0, entity_len-1]\n",
    "        chunk_ent_map = [\n",
    "            _find_closest_residue(entity_nums, chunk_ent_orig[0], True), \n",
    "            _find_closest_residue(entity_nums, chunk_ent_orig[1], False)\n",
    "        ]\n",
    "        chunk_pdb = [\n",
    "            pdb_residues[entity_to_pdb[chunk_ent_map[0]]].getResnum().item(),\n",
    "            pdb_residues[entity_to_pdb[chunk_ent_map[1]]].getResnum().item()\n",
    "        ]\n",
    "        domains = [OrderedDict(\n",
    "            domain_id=0,\n",
    "            manually_filled=True,\n",
    "            domain_chunks=[\n",
    "                OrderedDict(\n",
    "                    chunk_ent_orig=chunk_ent_orig,\n",
    "                    chunk_ent_map=chunk_ent_map,\n",
    "                    chunk_pdb_resid=chunk_pdb\n",
    "                )\n",
    "            ]\n",
    "        )]\n",
    "        \n",
    "    return domains\n",
    "\n",
    "\n",
    "def find_correct_instance(pdb_id, target_chain, chain_list):\n",
    "    for cid in chain_list:\n",
    "        url = f'https://data.rcsb.org/rest/v1/core/polymer_entity_instance/{pdb_id}/{cid}'\n",
    "        with urllib.request.urlopen(url) as f:\n",
    "            instance_data = json.loads(f.read().decode())   \n",
    "        if instance_data['rcsb_polymer_entity_instance_container_identifiers']['auth_asym_id'] == target_chain:\n",
    "            break\n",
    "    \n",
    "    if instance_data['rcsb_polymer_entity_instance_container_identifiers']['auth_asym_id'] != target_chain:\n",
    "        return None\n",
    "    \n",
    "    return instance_data\n",
    "\n",
    "\n",
    "def process_pdb_chain(outdir, pdb_id, chain):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    outdir = Path(outdir).mkdir_p()\n",
    "    ag, header = pdb_tools.get_atom_group(pdb_id, header=True)\n",
    "    \n",
    "    url = f'https://data.rcsb.org/rest/v1/core/entry/{pdb_id}'\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        entry_data = json.loads(f.read().decode())\n",
    "    \n",
    "    url = f'https://data.rcsb.org/rest/v1/core/polymer_entity_instance/{pdb_id}/{chain}'\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        instance_data = json.loads(f.read().decode())\n",
    "    \n",
    "    # yes, instance id can be different from the chain name, \n",
    "    # so we have to loop though all instances (chains) until \n",
    "    # we find the correct one\n",
    "    buf = instance_data['rcsb_polymer_entity_instance_container_identifiers']\n",
    "    if buf['asym_id'] != buf['auth_asym_id']:\n",
    "        instance_data = find_correct_instance(pdb_id, chain, set(ag.getChids()))\n",
    "    if instance_data is None:\n",
    "        raise RuntimeError('Cannot find correct RESTful link for {pdb_id}/{chain}. RCSB and Author\\'s namings are different')\n",
    "    \n",
    "    entity_id = instance_data['rcsb_polymer_entity_instance_container_identifiers']['entity_id']\n",
    "    \n",
    "    url = f'https://data.rcsb.org/rest/v1/core/polymer_entity/{pdb_id}/{entity_id}'\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        entity_data = json.loads(f.read().decode())\n",
    "    assert chain in entity_data['entity_poly']['pdbx_strand_id'].split(',')\n",
    "        \n",
    "    url = f'https://data.rcsb.org/rest/v1/core/uniprot/{pdb_id}/{entity_id}'\n",
    "    with urllib.request.urlopen(url) as f:\n",
    "        uniprot_data = json.loads(f.read().decode())\n",
    "        \n",
    "    # ensure there is no non-standard linkage and aas\n",
    "    assert entity_data['entity_poly']['nstd_linkage'] == 'no'\n",
    "    assert entity_data['entity_poly']['nstd_monomer'] == 'no'\n",
    "    \n",
    "    # align residues that have coordinates in the pdb to the \n",
    "    # sequence announced in the entity description and ID\n",
    "    # the missing residues as well as partially resolved residues\n",
    "    entity_seq = entity_data['entity_poly']['pdbx_seq_one_letter_code']\n",
    "    ag_chain = ag.select(f'chain {chain} and (stdaa or nonstdaa)').copy()\n",
    "    residues = list(ag_chain.getHierView().iterResidues())\n",
    "    pdb_seq = ''.join([three2one[x.getResname()] for x in residues])\n",
    "    entity_aln, pdb_aln = utils.global_align(entity_seq, pdb_seq)[0][:2]\n",
    "    \n",
    "    # sanity check\n",
    "    assert calc_identity(pdb_aln, entity_aln) > 0.90, 'Entity sequence doesn\\'t match the chain'\n",
    "    \n",
    "    entity_to_pdb = dict(_aln_to_mapping(entity_aln, pdb_aln))\n",
    "    missing_residue = []\n",
    "    missing_ca = []\n",
    "    missing_atoms = []\n",
    "    for ent_i, ent_res in enumerate(entity_seq):\n",
    "        if ent_i not in entity_to_pdb:\n",
    "            missing_residue.append(True)\n",
    "            missing_ca.append(True)\n",
    "            missing_atoms.append(True)\n",
    "            continue\n",
    "        missing_residue.append(False)\n",
    "        res = residues[entity_to_pdb[ent_i]]\n",
    "        res_ideal_names = set(residue_bonds_noh[res.getResname()].keys())\n",
    "        res_ideal_names.discard('OXT')\n",
    "        res_cur_names = set(res.heavy.getNames())\n",
    "        res_cur_names.discard('OXT')\n",
    "        missing_ca.append('CA' not in res_cur_names)\n",
    "        missing_atoms.append(res_ideal_names != res_cur_names)\n",
    "        \n",
    "    # write full pdb\n",
    "    prody.writePDB(outdir / f'{pdb_id}.pdb', ag)\n",
    "        \n",
    "    # ag_chain\n",
    "    prody.writePDB(outdir / 'rec_orig.pdb', ag_chain)\n",
    "        \n",
    "    # download alphafold model\n",
    "    uniprot_id = uniprot_data[0]['rcsb_id']\n",
    "    ag_af = fetch_AF_model(uniprot_id)\n",
    "    prody.writePDB(outdir / 'AF_orig.pdb', ag_af)\n",
    "    \n",
    "    # align AF to crystal\n",
    "    ag_af_aln, rmsd, (af_seq_aln, pdb_seq_aln) = pdb_tools.align(ag_af, ag_chain)\n",
    "    rmsd = rmsd[0]\n",
    "    prody.writePDB(outdir / 'AF_aln.pdb', ag_af_aln)\n",
    "    \n",
    "    AF_residues = list(ag_af.getHierView().iterResidues())\n",
    "    AF_seq = ''.join([three2one[x.getResname()] for x in AF_residues])\n",
    "    \n",
    "    # domain data\n",
    "    domains = fetch_domains(instance_data, entity_to_pdb, residues, len(entity_seq))\n",
    "    \n",
    "    entity_info = entity_data['entity_poly'].copy()\n",
    "    entity_info['entity_aln'] = entity_aln\n",
    "    entity_info['pdb_aln'] = pdb_aln\n",
    "    #print(entity_aln)\n",
    "    #print(pdb_aln)\n",
    "    \n",
    "    rec_dict = OrderedDict(\n",
    "        case_name=pdb_id + '_' + chain,\n",
    "        pdb_id=pdb_id,\n",
    "        pdb_chain=chain,\n",
    "        instance_id=instance_data['rcsb_polymer_entity_instance_container_identifiers']['asym_id'],\n",
    "        entity_id=entity_id,\n",
    "        experiment=header.get('experiment'), #entry_data['exptl']['method'],\n",
    "        resolution=header.get('resolution'), #entry_data['exptl']['method'],\n",
    "        deposition_date=entry_data['rcsb_accession_info']['deposit_date'],\n",
    "        \n",
    "        seqclus100=pdb_tools.CHAIN_TO_CLUSTER[100][pdb_id + '_' + chain],\n",
    "        seqclus90=pdb_tools.CHAIN_TO_CLUSTER[90][pdb_id + '_' + chain],\n",
    "        seqclus40=pdb_tools.CHAIN_TO_CLUSTER[40][pdb_id + '_' + chain],\n",
    "        seqclus30=pdb_tools.CHAIN_TO_CLUSTER[30][pdb_id + '_' + chain],\n",
    "        \n",
    "        missing_residue=''.join([str(int(x)) for x in missing_residue]),\n",
    "        missing_ca=''.join([str(int(x)) for x in missing_ca]),\n",
    "        missing_atoms=''.join([str(int(x)) for x in missing_atoms]),\n",
    "        \n",
    "        entity_info=entity_info,\n",
    "        \n",
    "        uniprot=OrderedDict(\n",
    "            uniprot_id=uniprot_id,\n",
    "            uniprot_seq=uniprot_data[0]['rcsb_uniprot_protein']['sequence'],\n",
    "            uniprot_name=uniprot_data[0]['rcsb_uniprot_protein']['name']['value'],\n",
    "        ),\n",
    "        \n",
    "        domains=domains,\n",
    "        \n",
    "        alphafold=OrderedDict(\n",
    "            seq=AF_seq,\n",
    "            PDB_CA_RMSD=rmsd,\n",
    "            AF_seq_aln=af_seq_aln,\n",
    "            PDB_seq_aln=pdb_seq_aln,\n",
    "            PDB_AF_identity=calc_identity(pdb_seq_aln, af_seq_aln),\n",
    "            AF_confidence=ag_af.calpha.getBetas().tolist()\n",
    "        )\n",
    "    )\n",
    "    return rec_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _add_hs(out_mol, in_mol):\n",
    "    # add hydrogens\n",
    "    mol = next(pybel.readfile('mol', in_mol))\n",
    "    mol.addh()\n",
    "    mol.localopt('mmff94', steps=500)\n",
    "    mol.write('mol', out_mol, overwrite=True)\n",
    "\n",
    "    # fix back the coordinates\n",
    "    mol = Chem.MolFromMolFile(out_mol, removeHs=False)\n",
    "    ag = utils.mol_to_ag(mol)\n",
    "    ref_ag = utils.mol_to_ag(Chem.MolFromMolFile(in_mol, removeHs=True))\n",
    "    tr = prody.calcTransformation(ag.heavy, ref_ag.heavy)\n",
    "    ag = tr.apply(ag)\n",
    "    prody.writePDB(Path(out_mol).stripext() + '.pdb', ag)\n",
    "\n",
    "    utils.change_mol_coords(mol, ag.getCoords())\n",
    "    AllChem.ComputeGasteigerCharges(mol, throwOnParamFailure=False)\n",
    "    Chem.MolToMolFile(mol, out_mol)\n",
    "\n",
    "\n",
    "def _calc_rmsd(crd1, crd2):\n",
    "    return np.square(crd1 - crd2).sum(1).mean().item()\n",
    "\n",
    "\n",
    "def _bsite_similarity(mol_ag, mob_ag, ref_ag, mob_aln, ref_aln):\n",
    "    ref_to_mob = dict(_aln_to_mapping(ref_aln, mob_aln))\n",
    "    mob_residues = list(mob_ag.copy().getHierView().iterResidues())\n",
    "    ref_residues = list(ref_ag.copy().getHierView().iterResidues())\n",
    "\n",
    "    assert len(mob_residues) == len(mob_aln.replace('-', ''))\n",
    "    assert len(ref_residues) == len(ref_aln.replace('-', ''))\n",
    "    \n",
    "    pocket = ref_ag.select(f'exwithin {BSITE_SIM_RADIUS} of sel', sel=mol_ag)\n",
    "    pocket_resnums = set(pocket.getResnums())\n",
    "    pocket_ref_residues = []\n",
    "    pocket_mob_residues = []\n",
    "    for i, x in enumerate(ref_residues):\n",
    "        if x.getResnum() in pocket_resnums:\n",
    "            pocket_ref_residues.append(x)\n",
    "            mob_id = ref_to_mob.get(i, None)\n",
    "            pocket_mob_residues.append(None if mob_id is None else mob_residues[mob_id])\n",
    "    \n",
    "    pocket_rmsd = None\n",
    "    if not all([x is None for x in pocket_mob_residues]):\n",
    "        pocket_rmsd = _calc_rmsd(\n",
    "            np.stack([x['CA'].getCoords() for i, x in enumerate(pocket_ref_residues) if pocket_mob_residues[i] is not None]),\n",
    "            np.stack([x['CA'].getCoords() for x in pocket_mob_residues if x is not None])\n",
    "        )\n",
    "    pocket_identity = sum([y is not None and x.getResname() == y.getResname() \n",
    "                           for x, y in zip(pocket_ref_residues, pocket_mob_residues)]) / len(pocket_ref_residues)\n",
    "    return OrderedDict(\n",
    "        nresidues=len(pocket_ref_residues),\n",
    "        rmsd_ca=pocket_rmsd,\n",
    "        identity=pocket_identity,\n",
    "        resnums_pdb=[x.getResnum().item() for x in pocket_ref_residues],\n",
    "        resnums_af=[x.getResnum().item() if x is not None else None for x in pocket_mob_residues],\n",
    "        seq_pdb=''.join([x['CA'].getSequence() for x in pocket_ref_residues]),\n",
    "        seq_af=''.join([x['CA'].getSequence() if x is not None else '-' for x in pocket_mob_residues])\n",
    "    )\n",
    "    \n",
    "\n",
    "def process_ligand_group(case_dir, case_dict, group_dict):\n",
    "    case_dir = Path(case_dir)\n",
    "    group_dir = case_dir / group_dict['name'] \n",
    "    \n",
    "    # add hs\n",
    "    for item in group_dict['ligands']:\n",
    "        _add_hs(group_dir / item['sdf_id'] + '_ah.mol', group_dir / item['sdf_id'] + '.mol')\n",
    "    \n",
    "    # make ligand group ag\n",
    "    mols = [(x['sdf_id'], utils.mol_to_ag(Chem.MolFromMolFile(group_dir / x['sdf_id'] + '.mol'))) for x in group_dict['ligands']]\n",
    "    combined_ag = mols[0][1].copy()\n",
    "    for _, mol in mols[1:]:\n",
    "        combined_ag += mol.copy()\n",
    "        \n",
    "    # define domains in contact\n",
    "    pdb_ag = prody.parsePDB(case_dir / 'rec_orig.pdb')\n",
    "    domains = case_dict['domains']\n",
    "    bsite_ag = pdb_ag.select(f'exwithin {LIGAND_DOMAIN_CONTACT_RADIUS} of lig', lig=combined_ag).copy()\n",
    "    interacting_domains = []\n",
    "    for dom in domains:\n",
    "        for chunk in dom['domain_chunks']:\n",
    "            a, b = chunk['chunk_pdb_resid']\n",
    "            if bsite_ag.select(f\"resnum `{a} to {b}`\") is not None:\n",
    "                interacting_domains.append(dom['domain_id'])\n",
    "                break\n",
    "    \n",
    "    #print(domains)\n",
    "    #print(interacting_domains)\n",
    "                \n",
    "    # select domains in pdb\n",
    "    resnums = []\n",
    "    for dom_id in interacting_domains:\n",
    "        dom = domains[dom_id]\n",
    "        for chunk in dom['domain_chunks']:\n",
    "            a, b = chunk['chunk_pdb_resid']\n",
    "            resnums.append((a, b))\n",
    "    #print('PD', ' or '.join([f'(resnum {a} to {b})' for a, b in resnums]))\n",
    "    domains_pdb_ag = pdb_ag.select(' or '.join([f\"(resnum `{a} to {b}`)\" for a, b in resnums])).copy()\n",
    "    prody.writePDB(group_dir / 'domains_crys.pdb', domains_pdb_ag)\n",
    "    pdb_resnums = resnums\n",
    "    \n",
    "    # select domains in AF\n",
    "    # align AF sequence to the entity sequence and select domain regions\n",
    "    af_ag = prody.parsePDB(case_dir / 'AF_orig.pdb')\n",
    "    entity_seq = case_dict['entity_info']['pdbx_seq_one_letter_code']\n",
    "    af_residues = list(af_ag.getHierView().iterResidues())\n",
    "    af_seq = ''.join([three2one[x.getResname()] for x in af_residues]) #case_dict['alphafold']['seq']\n",
    "    entity_aln, af_aln = utils.global_align(entity_seq, af_seq)[0][:2]\n",
    "    ent_to_af_map = dict(_aln_to_mapping(entity_aln, af_aln))\n",
    "    ent_resids = list(ent_to_af_map.keys())\n",
    "    #print(entity_aln)\n",
    "    #print(af_aln)\n",
    "    #print(ent_to_af_map)\n",
    "    #print(ent_resids)\n",
    "    #print('af_residues', len(af_residues))\n",
    "    #print('af_seq', len(af_seq))\n",
    "    resnums = []\n",
    "    for dom_id in interacting_domains:\n",
    "        dom = domains[dom_id]\n",
    "        #print(dom)\n",
    "        for chunk in dom['domain_chunks']:\n",
    "            a, b = chunk['chunk_ent_orig']\n",
    "            #print(a, b)\n",
    "            #print(ent_to_af_map[_find_closest_residue(ent_resids, a, True)], \n",
    "            #      ent_to_af_map[_find_closest_residue(ent_resids, b, False)])\n",
    "            a = af_residues[ent_to_af_map[_find_closest_residue(ent_resids, a, True)]].getResnum().item()\n",
    "            b = af_residues[ent_to_af_map[_find_closest_residue(ent_resids, b, False)]].getResnum().item()\n",
    "            resnums.append((a, b))\n",
    "    #print('AF', ' or '.join([f'(resnum {a} to {b})' for a, b in resnums]))\n",
    "    domains_af_ag = af_ag.select(' or '.join([f\"(resnum `{a} to {b}`)\" for a, b in resnums])).copy()\n",
    "    prody.writePDB(group_dir / 'domains_AF.pdb', domains_af_ag)\n",
    "    af_resnums = resnums\n",
    "    \n",
    "    # align AF domain to PDB domain\n",
    "    domains_af_ag_aln, rmsd, (af_aln, pdb_aln) = pdb_tools.align(domains_af_ag, domains_pdb_ag)\n",
    "    rmsd = rmsd[0]\n",
    "    prody.writePDB(group_dir / 'domains_AF_aln.pdb', domains_af_ag_aln)\n",
    "    \n",
    "    domain_aligment = OrderedDict(\n",
    "        interacting_domains=interacting_domains,\n",
    "        pdb_resi_ranges=pdb_resnums,\n",
    "        af_resi_ranges=af_resnums,\n",
    "        af_aln=af_aln,\n",
    "        pdb_aln=pdb_aln,\n",
    "        pdb_af_identity=calc_identity(pdb_aln, af_aln),\n",
    "        rmsd_aln=rmsd\n",
    "    )\n",
    "    \n",
    "    bsite_analysis = _bsite_similarity(combined_ag, domains_af_ag_aln, domains_pdb_ag, af_aln, pdb_aln)\n",
    "    return domain_aligment, bsite_analysis\n",
    "    \n",
    "    \n",
    "#process_ligand_group('data/3WKE_A', case_dict, case_dict['ligand_groups'][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get_cases_for_pdb('6bg0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_chain(chemid, mol_rd, pdb_ag):\n",
    "    mol_ag = utils.mol_to_ag(mol_rd)\n",
    "    not_lig = pdb_ag.select('not resname ' + chemid).copy()\n",
    "    \n",
    "    pocket_all = not_lig.select(f'(not water) exwithin {CHAIN_INTERACTION_DIST} of sel', sel=mol_ag)\n",
    "    assert pocket_all is not None\n",
    "    assert len(pocket_all) > MIN_POCKET_SIZE\n",
    "    \n",
    "    dmat_min = np.sqrt(_build_d2mat(pocket_all.getCoords(), mol_ag.getCoords())).min()\n",
    "    #print(dmat_min)\n",
    "    assert dmat_min > COVALENT_LIGAND_DISTANCE, f'{dmat_min} <= {COVALENT_LIGAND_DISTANCE}'\n",
    "    \n",
    "    # exclude hetero because ligands in the same group can be in different chains\n",
    "    # and count toward \"counts\"\n",
    "    pocket = pocket_all.select('not hetero')\n",
    "    counts = Counter(pocket.getChids())\n",
    "    chain = sorted(counts.items(), key=lambda x: -x[1])[0][0]\n",
    "    assert (counts[chain] / len(pocket)) > 0.8\n",
    "    return chain\n",
    "\n",
    "\n",
    "def assert_one_chemid_copy_per_chain(chemid, pdb_ag):\n",
    "    for chain_ag in pdb_ag.getHierView().iterChains():\n",
    "        chain_ag = chain_ag.protein\n",
    "        if chain_ag is None:\n",
    "            continue\n",
    "        lig = pdb_ag.select(f'resname {chemid} within 6 of sel', sel=chain_ag)\n",
    "        if lig is not None:\n",
    "            assert len(set(lig.getResnums())) == 1\n",
    "            \n",
    "\n",
    "def get_crystall_lattice(pdb, cutoff=6):\n",
    "    pdb_file = pdb_tools.pdb_file_path(pdb)\n",
    "    cmd.reinitialize()\n",
    "    cmd.load(pdb_file, 'orig')\n",
    "    cmd.symexp('symm', 'orig', 'orig', cutoff)\n",
    "    tmp_file = utils.tmp_file(prefix='alphadock-', suffix='.pdb')\n",
    "    cmd.save(tmp_file, 'symm*')\n",
    "    symm_ag = prody.parsePDB(tmp_file)\n",
    "    Path(tmp_file).remove_p()\n",
    "    cmd.delete('all')\n",
    "    return symm_ag\n",
    "\n",
    "\n",
    "#get_crystall_lattice('4ow1')\n",
    "\n",
    "\n",
    "def get_cases_for_pdb(pdb_id):\n",
    "    pdb_id = pdb_id.upper()\n",
    "    wdir = Path('data').mkdir_p().mkdir_p()\n",
    "    \n",
    "    try:\n",
    "        db_result = pdb_tools.get_atom_group(pdb_id, header=True)\n",
    "        assert db_result is not None\n",
    "        pdb_ag, header = db_result\n",
    "        assert header.get('resolution', MAX_RESOLUTION + 10) <= MAX_RESOLUTION\n",
    "        # avoid crazy large pdb files like ribosomes\n",
    "        num_chains = len(set(pdb_ag.getChids()))\n",
    "        assert num_chains <= MAX_NUM_CHAINS, f'{num_chains} > {MAX_NUM_CHAINS}'\n",
    "    except AssertionError as e:\n",
    "        logger.warning(f'{pdb_id} does not pass the criteria')\n",
    "        logger.exception(e)\n",
    "        return []\n",
    "    \n",
    "    chemids = LigandExpo.get_chemids_list(pdb_id)\n",
    "    \n",
    "    # find groups of ligands. here we treat groups of proximal ligands as \n",
    "    # whole instead of discarding them. such cases will represent cooperative binding\n",
    "    mol_sdf_ids = []\n",
    "    mol_rds = {}\n",
    "    mol_rd_coords = {}\n",
    "    for chemid in chemids:\n",
    "        sdf_ids = LigandExpo.get_all_sdf_ids(pdb_id, chemid)\n",
    "        for sdf_id in sdf_ids:\n",
    "            mol_rd = LigandExpo.get_all_sdf_mol(sdf_id)\n",
    "            if mol_rd is not None:\n",
    "                mol_rds[sdf_id] = mol_rd\n",
    "                mol_rd_coords[sdf_id] = mol_rd.GetConformer(0).GetPositions()\n",
    "                mol_sdf_ids.append(sdf_id)\n",
    "            else:\n",
    "                logger.warning(f'Cannot read {sdf_id}')\n",
    "    \n",
    "    mol_groups = []\n",
    "    for sdf_a in mol_sdf_ids:\n",
    "        crd_a = mol_rd_coords[sdf_a]\n",
    "        found = None\n",
    "        for mol_group in mol_groups:\n",
    "            for sdf_b in mol_group:\n",
    "                crd_b = mol_rd_coords[sdf_b]\n",
    "                d2mat = np.sum((crd_a[:, None, :] - crd_b[None, :, :])**2, axis=2)\n",
    "                if np.any(d2mat < LIGAND_GROUP_DIST**2):\n",
    "                    found = mol_group\n",
    "                    break\n",
    "            if found is not None:\n",
    "                break\n",
    "        if found is not None:\n",
    "            found.append(sdf_a)\n",
    "        else:\n",
    "            mol_groups.append([sdf_a])\n",
    "            \n",
    "    #print(mol_groups)\n",
    "            \n",
    "    # build symmetry mates\n",
    "    symm_mates_ag = get_crystall_lattice(pdb_id)\n",
    "    \n",
    "    # for each sdf_id\n",
    "    detailed_groups = []\n",
    "    for mol_group in mol_groups:\n",
    "        detailed_group = []\n",
    "        # if at least one ligand in the group is bad, we skip the whole group\n",
    "        try:\n",
    "            for sdf_id in mol_group:\n",
    "                mol_rd = mol_rds[sdf_id]\n",
    "                #assert mol_rd is not None\n",
    "                \n",
    "                chemid = sdf_id.split('_')[1]\n",
    "                assert_one_chemid_copy_per_chain(chemid, pdb_ag)\n",
    "                \n",
    "                # check smiles and that all heavy atoms are present\n",
    "                smi = LigandExpo.get_smiles(chemid)\n",
    "                assert smi is not None\n",
    "                \n",
    "                mol_smi = Chem.MolFromSmiles(smi)\n",
    "                nheavy = mol_smi.GetNumHeavyAtoms()\n",
    "                frac_resolved = nheavy / mol_rd.GetNumHeavyAtoms()\n",
    "                assert frac_resolved >= FRAC_HEAVY_ATOMS_RESOLVED, f'{nheavy} < {mol_rd.GetNumHeavyAtoms()}, ({smi})'\n",
    "                \n",
    "                # check that ligand interacts with a single chain\n",
    "                pdb_chain = get_chain(chemid, mol_rd, pdb_ag)\n",
    "                \n",
    "                # check that there is no symm mates interaction\n",
    "                mol_ag = utils.mol_to_ag(mol_rd)\n",
    "                if symm_mates_ag is not None:\n",
    "                    assert mol_ag.select(f'within {MIN_LIGAND_TO_SYMMATES} of symmmates', symmmates=symm_mates_ag) is None\n",
    "                \n",
    "                case = OrderedDict(\n",
    "                    sdf_id=sdf_id, \n",
    "                    chemid=chemid,\n",
    "                    smiles=smi,\n",
    "                    pdb_id=pdb_id,\n",
    "                    pdb_chain=pdb_chain,\n",
    "                    num_heavy_atoms=mol_rd.GetNumHeavyAtoms(),\n",
    "                    frac_resolved=frac_resolved,\n",
    "                    affinity=_get_affinity(pdb_id, chemid)\n",
    "                )\n",
    "                detailed_group.append(case)\n",
    "                \n",
    "            # check that all chains in the group are the same\n",
    "            inter_chains = set([x['pdb_chain'] for x in detailed_group])\n",
    "            assert len(inter_chains) == 1, f'Multiple interacting chains: {inter_chains}'\n",
    "            \n",
    "        except AssertionError as e:\n",
    "            logger.info(f'Dropping group {mol_group}')\n",
    "            logger.exception(e)\n",
    "            continue\n",
    "            \n",
    "        detailed_groups.append(detailed_group)\n",
    "        \n",
    "    if len(detailed_groups) == 0:\n",
    "        logger.error('Groups are empty')\n",
    "        return []\n",
    "        \n",
    "    # select unique (chain, chemids) groups\n",
    "    unique_groups = []\n",
    "    _desc_list = []\n",
    "    for group in detailed_groups:\n",
    "        seqclus100 = pdb_tools.CHAIN_TO_CLUSTER[100].get(pdb_id + '_' + group[0]['pdb_chain'])\n",
    "        if seqclus100 is None:\n",
    "            continue\n",
    "        _desc = [seqclus100]\n",
    "        _desc += sorted([x['chemid'] for x in group])\n",
    "        _desc = tuple(_desc)\n",
    "        if _desc in _desc_list:\n",
    "            continue\n",
    "        _desc_list.append(_desc)\n",
    "        unique_groups.append(group)\n",
    "        \n",
    "    # prepare dict with chains\n",
    "    chains_dict = OrderedDict()\n",
    "    chains_list = sorted(set([x[0]['pdb_chain'] for x in unique_groups]))\n",
    "    for chain in chains_list:\n",
    "        try:\n",
    "            chain_name = pdb_id + '_' + chain\n",
    "            chains_dict[chain_name] = process_pdb_chain(wdir / chain_name, pdb_id, chain)\n",
    "            chains_dict[chain_name]['ligand_groups'] = []\n",
    "        except Exception as e:\n",
    "            (wdir / chain_name).rmtree_p()\n",
    "            logger.warning(f'Cannot process {chain_name}, skipping')\n",
    "            logger.exception(e)\n",
    "            continue\n",
    "    \n",
    "    # process ligand groups\n",
    "    for group in unique_groups:\n",
    "        try:\n",
    "            chain_name = pdb_id + '_' + group[0]['pdb_chain']\n",
    "            chain_dict = chains_dict.get(chain_name)\n",
    "            if chain_dict is None:\n",
    "                continue\n",
    "\n",
    "            group_name = '_'.join(sorted([x['chemid'] for x in group]))\n",
    "            group_dir = (wdir / chain_name / group_name).mkdir_p()\n",
    "            group_dict = OrderedDict(\n",
    "                name=group_name,\n",
    "                ligands=group\n",
    "            )\n",
    "\n",
    "            for lig_dict in group:\n",
    "                Chem.MolToMolFile(mol_rds[lig_dict['sdf_id']], group_dir / lig_dict['sdf_id'] + '.mol')\n",
    "\n",
    "            domain_aligment, bsite_analysis = process_ligand_group(wdir / chain_name, chain_dict, group_dict)\n",
    "            group_dict['domain_aligment'] = domain_aligment\n",
    "            group_dict['bsite_analysis'] = bsite_analysis\n",
    "            chain_dict['ligand_groups'].append(group_dict)\n",
    "        except Exception as e:\n",
    "            logger.warning(f'Cannot process {group_name}, skipping')\n",
    "            logger.exception(e)\n",
    "        \n",
    "    cases = []\n",
    "    for case in chains_dict.values():\n",
    "        # if all ligand groups were discarded skip this chain\n",
    "        if len(case['ligand_groups']) == 0:\n",
    "            (wdir / case['case_name']).rmtree_p()\n",
    "            continue\n",
    "        utils.write_json(case, wdir / case['case_name'] / 'case.json')\n",
    "        cases.append(case)\n",
    "        \n",
    "    return cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#case_dicts = get_cases_for_pdb('3wke')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#case_dicts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set(['a',24]) in [set([24,'a'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#LigandExpo._PDB_TO_CC #.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    old_h = logger.handlers\n",
    "    logger.handlers = [get_file_handler(Path('data') / 'log.txt')]\n",
    "    for pdb in tqdm(list(LigandExpo._PDB_TO_CC.keys())):\n",
    "        logger.info(f'============ Processing {pdb} ===========')\n",
    "        get_cases_for_pdb(pdb)\n",
    "        logger.info(f'============  Finished {pdb}  ===========\\n\\n')\n",
    "finally:\n",
    "    logger.handlers[0].close()\n",
    "    logger.handlers = old_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cases_for_pdb('5A1P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cases_for_pdb('5yqb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chem.MolFromSmiles('[H]/N=C(\\c1ccc(cc1)C[C@H](C(=O)N2CCCCC2)NC(=O)[C@H](CCC(=O)O)NS(=O)(=O)c3ccc4ccccc4c3)/N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_mp(args):\n",
    "    chemid, pdb = args\n",
    "    res = {'chemid': chemid, 'pdb': pdb}\n",
    "    try:\n",
    "        cases = get_cases(chemid, pdb)\n",
    "        res['cases'] = cases\n",
    "    except Exception:\n",
    "        res.update({'exc': traceback.format_exc()})\n",
    "    return res\n",
    "\n",
    "\n",
    "# In[194]:\n",
    "\n",
    "\n",
    "def get_args():\n",
    "    for chemid, pdbs in LigandExpo._CC_TO_PDB.items():\n",
    "        for pdb in pdbs:\n",
    "            yield chemid, pdb\n",
    "\n",
    "with Pool(32) as p:\n",
    "    ntotal = sum(len(x) for x in LigandExpo._CC_TO_PDB.values())\n",
    "    for r in tqdm(p.imap_unordered(run_mp, get_args()), total=ntotal):\n",
    "        if 'exc' in r:\n",
    "            print(r)\n",
    "            print(r['exc'])\n",
    "            continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pocket_dock",
   "language": "python",
   "name": "pocket_dock"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
